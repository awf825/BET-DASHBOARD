{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9c3b57",
   "metadata": {},
   "source": [
    "# MLB 2025 Baselines (Paper-aligned): Logistic Regression + SVM\n",
    "\n",
    "This notebook implements **Steps 1–8** to train **Logistic Regression** and **SVM** baselines on your `games_table_2025` table, using a **time-based cutoff** and dropping the columns we agreed to exclude.\n",
    "\n",
    "**Assumptions**\n",
    "- Your SQLite DB file is `mlb_scrape.sqlite`\n",
    "- You have already built: `games_table_2025` (via step3)\n",
    "- Cutoff date: **2025-04-07** (inclusive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5f75de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports + config\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "DB_PATH = \"mlb_scrape.sqlite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c754f811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found tables: ['games_table_2015', 'games_table_2016', 'games_table_2017', 'games_table_2018', 'games_table_2019', 'games_table_2020', 'games_table_2021', 'games_table_2022', 'games_table_2023', 'games_table_2024', 'games_table_2025']\n",
      "Combined rows: 25193\n",
      "park_pf_runs non-null: 24836 / 25193\n",
      "home_sp_throws non-null: 25193 / 25193\n",
      "away_sp_throws non-null: 25193 / 25193\n",
      "Rows after Apr 7 cutoff (all seasons): 24437\n",
      "['gamePk', 'season', 'gameDate', 'homeTeamId', 'awayTeamId', 'homeTeamName', 'awayTeamName', 'homeWin', 'home_bat_season_B1_AB_mean', 'home_bat_season_B1_AB_std', 'home_bat_season_B2_H_mean', 'home_bat_season_B2_H_std', 'home_bat_season_B3_BB_mean', 'home_bat_season_B3_BB_std', 'home_bat_season_B4_SO_mean', 'home_bat_season_B4_SO_std', 'home_bat_season_B5_PA_mean', 'home_bat_season_B5_PA_std', 'home_bat_season_B6_BA_mean', 'home_bat_season_B6_BA_std', 'home_bat_season_B7_OBP_mean', 'home_bat_season_B7_OBP_std', 'home_bat_season_B8_SLG_mean', 'home_bat_season_B8_SLG_std', 'home_bat_season_B9_OPS_mean', 'home_bat_season_B9_OPS_std', 'home_bat_season_B10_Pit_mean', 'home_bat_season_B10_Pit_std', 'home_bat_season_B11_Str_mean', 'home_bat_season_B11_Str_std', 'home_bat_season_B12_PO_mean', 'home_bat_season_B12_PO_std', 'home_bat_season_B13_A_mean', 'home_bat_season_B13_A_std', 'home_bat_season_B14_wRCplus_mean', 'home_bat_season_B14_wRCplus_std', 'home_bat_season_B15_HardHitPct_mean', 'home_bat_season_B15_HardHitPct_std', 'home_bat_season_B16_BarrelPct_mean', 'home_bat_season_B16_BarrelPct_std', 'home_bat_season_B17_OPSplus_mean', 'home_bat_season_B17_OPSplus_std', 'home_bat_last10_B1_AB_mean', 'home_bat_last10_B1_AB_std', 'home_bat_last10_B2_H_mean', 'home_bat_last10_B2_H_std', 'home_bat_last10_B3_BB_mean', 'home_bat_last10_B3_BB_std', 'home_bat_last10_B4_SO_mean', 'home_bat_last10_B4_SO_std', 'home_bat_last10_B5_PA_mean', 'home_bat_last10_B5_PA_std', 'home_bat_last10_B6_BA_mean', 'home_bat_last10_B6_BA_std', 'home_bat_last10_B7_OBP_mean', 'home_bat_last10_B7_OBP_std', 'home_bat_last10_B8_SLG_mean', 'home_bat_last10_B8_SLG_std', 'home_bat_last10_B9_OPS_mean', 'home_bat_last10_B9_OPS_std', 'home_bat_last10_B10_Pit_mean', 'home_bat_last10_B10_Pit_std', 'home_bat_last10_B11_Str_mean', 'home_bat_last10_B11_Str_std', 'home_bat_last10_B12_PO_mean', 'home_bat_last10_B12_PO_std', 'home_bat_last10_B13_A_mean', 'home_bat_last10_B13_A_std', 'home_bat_last10_B14_wRCplus_mean', 'home_bat_last10_B14_wRCplus_std', 'home_bat_last10_B15_HardHitPct_mean', 'home_bat_last10_B15_HardHitPct_std', 'home_bat_last10_B16_BarrelPct_mean', 'home_bat_last10_B16_BarrelPct_std', 'home_bat_last10_B17_OPSplus_mean', 'home_bat_last10_B17_OPSplus_std', 'home_bat_last20_B1_AB_mean', 'home_bat_last20_B1_AB_std', 'home_bat_last20_B2_H_mean', 'home_bat_last20_B2_H_std', 'home_bat_last20_B3_BB_mean', 'home_bat_last20_B3_BB_std', 'home_bat_last20_B4_SO_mean', 'home_bat_last20_B4_SO_std', 'home_bat_last20_B5_PA_mean', 'home_bat_last20_B5_PA_std', 'home_bat_last20_B6_BA_mean', 'home_bat_last20_B6_BA_std', 'home_bat_last20_B7_OBP_mean', 'home_bat_last20_B7_OBP_std', 'home_bat_last20_B8_SLG_mean', 'home_bat_last20_B8_SLG_std', 'home_bat_last20_B9_OPS_mean', 'home_bat_last20_B9_OPS_std', 'home_bat_last20_B10_Pit_mean', 'home_bat_last20_B10_Pit_std', 'home_bat_last20_B11_Str_mean', 'home_bat_last20_B11_Str_std', 'home_bat_last20_B12_PO_mean', 'home_bat_last20_B12_PO_std', 'home_bat_last20_B13_A_mean', 'home_bat_last20_B13_A_std', 'home_bat_last20_B14_wRCplus_mean', 'home_bat_last20_B14_wRCplus_std', 'home_bat_last20_B15_HardHitPct_mean', 'home_bat_last20_B15_HardHitPct_std', 'home_bat_last20_B16_BarrelPct_mean', 'home_bat_last20_B16_BarrelPct_std', 'home_bat_last20_B17_OPSplus_mean', 'home_bat_last20_B17_OPSplus_std', 'away_bat_season_B1_AB_mean', 'away_bat_season_B1_AB_std', 'away_bat_season_B2_H_mean', 'away_bat_season_B2_H_std', 'away_bat_season_B3_BB_mean', 'away_bat_season_B3_BB_std', 'away_bat_season_B4_SO_mean', 'away_bat_season_B4_SO_std', 'away_bat_season_B5_PA_mean', 'away_bat_season_B5_PA_std', 'away_bat_season_B6_BA_mean', 'away_bat_season_B6_BA_std', 'away_bat_season_B7_OBP_mean', 'away_bat_season_B7_OBP_std', 'away_bat_season_B8_SLG_mean', 'away_bat_season_B8_SLG_std', 'away_bat_season_B9_OPS_mean', 'away_bat_season_B9_OPS_std', 'away_bat_season_B10_Pit_mean', 'away_bat_season_B10_Pit_std', 'away_bat_season_B11_Str_mean', 'away_bat_season_B11_Str_std', 'away_bat_season_B12_PO_mean', 'away_bat_season_B12_PO_std', 'away_bat_season_B13_A_mean', 'away_bat_season_B13_A_std', 'away_bat_season_B14_wRCplus_mean', 'away_bat_season_B14_wRCplus_std', 'away_bat_season_B15_HardHitPct_mean', 'away_bat_season_B15_HardHitPct_std', 'away_bat_season_B16_BarrelPct_mean', 'away_bat_season_B16_BarrelPct_std', 'away_bat_season_B17_OPSplus_mean', 'away_bat_season_B17_OPSplus_std', 'away_bat_last10_B1_AB_mean', 'away_bat_last10_B1_AB_std', 'away_bat_last10_B2_H_mean', 'away_bat_last10_B2_H_std', 'away_bat_last10_B3_BB_mean', 'away_bat_last10_B3_BB_std', 'away_bat_last10_B4_SO_mean', 'away_bat_last10_B4_SO_std', 'away_bat_last10_B5_PA_mean', 'away_bat_last10_B5_PA_std', 'away_bat_last10_B6_BA_mean', 'away_bat_last10_B6_BA_std', 'away_bat_last10_B7_OBP_mean', 'away_bat_last10_B7_OBP_std', 'away_bat_last10_B8_SLG_mean', 'away_bat_last10_B8_SLG_std', 'away_bat_last10_B9_OPS_mean', 'away_bat_last10_B9_OPS_std', 'away_bat_last10_B10_Pit_mean', 'away_bat_last10_B10_Pit_std', 'away_bat_last10_B11_Str_mean', 'away_bat_last10_B11_Str_std', 'away_bat_last10_B12_PO_mean', 'away_bat_last10_B12_PO_std', 'away_bat_last10_B13_A_mean', 'away_bat_last10_B13_A_std', 'away_bat_last10_B14_wRCplus_mean', 'away_bat_last10_B14_wRCplus_std', 'away_bat_last10_B15_HardHitPct_mean', 'away_bat_last10_B15_HardHitPct_std', 'away_bat_last10_B16_BarrelPct_mean', 'away_bat_last10_B16_BarrelPct_std', 'away_bat_last10_B17_OPSplus_mean', 'away_bat_last10_B17_OPSplus_std', 'away_bat_last20_B1_AB_mean', 'away_bat_last20_B1_AB_std', 'away_bat_last20_B2_H_mean', 'away_bat_last20_B2_H_std', 'away_bat_last20_B3_BB_mean', 'away_bat_last20_B3_BB_std', 'away_bat_last20_B4_SO_mean', 'away_bat_last20_B4_SO_std', 'away_bat_last20_B5_PA_mean', 'away_bat_last20_B5_PA_std', 'away_bat_last20_B6_BA_mean', 'away_bat_last20_B6_BA_std', 'away_bat_last20_B7_OBP_mean', 'away_bat_last20_B7_OBP_std', 'away_bat_last20_B8_SLG_mean', 'away_bat_last20_B8_SLG_std', 'away_bat_last20_B9_OPS_mean', 'away_bat_last20_B9_OPS_std', 'away_bat_last20_B10_Pit_mean', 'away_bat_last20_B10_Pit_std', 'away_bat_last20_B11_Str_mean', 'away_bat_last20_B11_Str_std', 'away_bat_last20_B12_PO_mean', 'away_bat_last20_B12_PO_std', 'away_bat_last20_B13_A_mean', 'away_bat_last20_B13_A_std', 'away_bat_last20_B14_wRCplus_mean', 'away_bat_last20_B14_wRCplus_std', 'away_bat_last20_B15_HardHitPct_mean', 'away_bat_last20_B15_HardHitPct_std', 'away_bat_last20_B16_BarrelPct_mean', 'away_bat_last20_B16_BarrelPct_std', 'away_bat_last20_B17_OPSplus_mean', 'away_bat_last20_B17_OPSplus_std', 'home_sp_season_SP1_IP_mean', 'home_sp_season_SP1_IP_std', 'home_sp_season_SP2_H_mean', 'home_sp_season_SP2_H_std', 'home_sp_season_SP3_BB_mean', 'home_sp_season_SP3_BB_std', 'home_sp_season_SP4_SO_mean', 'home_sp_season_SP4_SO_std', 'home_sp_season_SP5_HR_mean', 'home_sp_season_SP5_HR_std', 'home_sp_season_SP6_ERA_mean', 'home_sp_season_SP6_ERA_std', 'home_sp_season_SP7_BF_mean', 'home_sp_season_SP7_BF_std', 'home_sp_season_SP8_Pit_mean', 'home_sp_season_SP8_Pit_std', 'home_sp_season_SP9_Str_mean', 'home_sp_season_SP9_Str_std', 'home_sp_season_WHIP_mean', 'home_sp_season_WHIP_std', 'home_sp_last3_SP1_IP_mean', 'home_sp_last3_SP1_IP_std', 'home_sp_last3_SP2_H_mean', 'home_sp_last3_SP2_H_std', 'home_sp_last3_SP3_BB_mean', 'home_sp_last3_SP3_BB_std', 'home_sp_last3_SP4_SO_mean', 'home_sp_last3_SP4_SO_std', 'home_sp_last3_SP5_HR_mean', 'home_sp_last3_SP5_HR_std', 'home_sp_last3_SP6_ERA_mean', 'home_sp_last3_SP6_ERA_std', 'home_sp_last3_SP7_BF_mean', 'home_sp_last3_SP7_BF_std', 'home_sp_last3_SP8_Pit_mean', 'home_sp_last3_SP8_Pit_std', 'home_sp_last3_SP9_Str_mean', 'home_sp_last3_SP9_Str_std', 'home_sp_last3_WHIP_mean', 'home_sp_last3_WHIP_std', 'away_sp_season_SP1_IP_mean', 'away_sp_season_SP1_IP_std', 'away_sp_season_SP2_H_mean', 'away_sp_season_SP2_H_std', 'away_sp_season_SP3_BB_mean', 'away_sp_season_SP3_BB_std', 'away_sp_season_SP4_SO_mean', 'away_sp_season_SP4_SO_std', 'away_sp_season_SP5_HR_mean', 'away_sp_season_SP5_HR_std', 'away_sp_season_SP6_ERA_mean', 'away_sp_season_SP6_ERA_std', 'away_sp_season_SP7_BF_mean', 'away_sp_season_SP7_BF_std', 'away_sp_season_SP8_Pit_mean', 'away_sp_season_SP8_Pit_std', 'away_sp_season_SP9_Str_mean', 'away_sp_season_SP9_Str_std', 'away_sp_season_WHIP_mean', 'away_sp_season_WHIP_std', 'away_sp_last3_SP1_IP_mean', 'away_sp_last3_SP1_IP_std', 'away_sp_last3_SP2_H_mean', 'away_sp_last3_SP2_H_std', 'away_sp_last3_SP3_BB_mean', 'away_sp_last3_SP3_BB_std', 'away_sp_last3_SP4_SO_mean', 'away_sp_last3_SP4_SO_std', 'away_sp_last3_SP5_HR_mean', 'away_sp_last3_SP5_HR_std', 'away_sp_last3_SP6_ERA_mean', 'away_sp_last3_SP6_ERA_std', 'away_sp_last3_SP7_BF_mean', 'away_sp_last3_SP7_BF_std', 'away_sp_last3_SP8_Pit_mean', 'away_sp_last3_SP8_Pit_std', 'away_sp_last3_SP9_Str_mean', 'away_sp_last3_SP9_Str_std', 'away_sp_last3_WHIP_mean', 'away_sp_last3_WHIP_std', 'home_rp_season_P1_IP_mean', 'home_rp_season_P1_IP_std', 'home_rp_season_P2_H_mean', 'home_rp_season_P2_H_std', 'home_rp_season_P3_BB_mean', 'home_rp_season_P3_BB_std', 'home_rp_season_P4_SO_mean', 'home_rp_season_P4_SO_std', 'home_rp_season_P5_HR_mean', 'home_rp_season_P5_HR_std', 'home_rp_season_P6_ERA_mean', 'home_rp_season_P6_ERA_std', 'home_rp_season_P7_BF_mean', 'home_rp_season_P7_BF_std', 'home_rp_season_P8_Pit_mean', 'home_rp_season_P8_Pit_std', 'home_rp_season_P9_Str_mean', 'home_rp_season_P9_Str_std', 'home_rp_season_P17_IR_mean', 'home_rp_season_P17_IR_std', 'home_rp_season_P18_IS_mean', 'home_rp_season_P18_IS_std', 'away_rp_season_P1_IP_mean', 'away_rp_season_P1_IP_std', 'away_rp_season_P2_H_mean', 'away_rp_season_P2_H_std', 'away_rp_season_P3_BB_mean', 'away_rp_season_P3_BB_std', 'away_rp_season_P4_SO_mean', 'away_rp_season_P4_SO_std', 'away_rp_season_P5_HR_mean', 'away_rp_season_P5_HR_std', 'away_rp_season_P6_ERA_mean', 'away_rp_season_P6_ERA_std', 'away_rp_season_P7_BF_mean', 'away_rp_season_P7_BF_std', 'away_rp_season_P8_Pit_mean', 'away_rp_season_P8_Pit_std', 'away_rp_season_P9_Str_mean', 'away_rp_season_P9_Str_std', 'away_rp_season_P17_IR_mean', 'away_rp_season_P17_IR_std', 'away_rp_season_P18_IS_mean', 'away_rp_season_P18_IS_std', 'home_sp_career_SP1_IP', 'home_sp_career_SP2_H', 'home_sp_career_SP3_BB', 'home_sp_career_SP4_SO', 'home_sp_career_SP5_HR', 'home_sp_career_SP6_ERA', 'home_sp_career_SP7_BF', 'home_sp_career_SP8_Pit', 'home_sp_career_SP9_Str', 'home_sp_career_WHIP', 'away_sp_career_SP1_IP', 'away_sp_career_SP2_H', 'away_sp_career_SP3_BB', 'away_sp_career_SP4_SO', 'away_sp_career_SP5_HR', 'away_sp_career_SP6_ERA', 'away_sp_career_SP7_BF', 'away_sp_career_SP8_Pit', 'away_sp_career_SP9_Str', 'away_sp_career_WHIP', 'park_pf_runs', 'home_sp_throws', 'away_sp_throws', 'gameDate_dt']\n"
     ]
    }
   ],
   "source": [
    "START_YEAR, END_YEAR = 2015, 2025\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# (Optional) verify which tables exist\n",
    "existing = set(r[0] for r in conn.execute(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'games_table_%'\"\n",
    ").fetchall())\n",
    "\n",
    "tables = [f\"games_table_{y}\" for y in range(START_YEAR, END_YEAR + 1) if f\"games_table_{y}\" in existing]\n",
    "print(\"Found tables:\", tables)\n",
    "\n",
    "union_sql = \"\\nUNION ALL\\n\".join([f\"SELECT * FROM {t}\" for t in tables])\n",
    "union_sql = f\"\"\"\n",
    "SELECT *\n",
    "FROM (\n",
    "{union_sql}\n",
    ")\n",
    "ORDER BY gameDate ASC\n",
    "\"\"\"\n",
    "\n",
    "df_all = pd.read_sql(union_sql, conn)\n",
    "print(\"Combined rows:\", len(df_all))\n",
    "\n",
    "# ---- NEW: merge park factors from context_game (by gamePk) ----\n",
    "pf = pd.read_sql(\"SELECT gamePk, park_pf_runs FROM context_game\", conn)\n",
    "df_all = df_all.merge(pf, on=\"gamePk\", how=\"left\")\n",
    "print(\"park_pf_runs non-null:\", df_all[\"park_pf_runs\"].notnull().sum(), \"/\", len(df_all))\n",
    "\n",
    "# weather = pd.read_sql(\n",
    "#     \"\"\"\n",
    "#     SELECT gamePk,\n",
    "#            temp_f,\n",
    "#            wind_mph,\n",
    "#            humidity,\n",
    "#            precip_mm\n",
    "#     FROM context_game\n",
    "#     \"\"\",\n",
    "#     conn\n",
    "# )\n",
    "\n",
    "# df_all = df_all.merge(weather, on=\"gamePk\", how=\"left\")\n",
    "\n",
    "hand = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT gamePk, home_sp_throws, away_sp_throws\n",
    "    FROM context_game\n",
    "    \"\"\",\n",
    "    conn\n",
    ")\n",
    "df_all = df_all.merge(hand, on=\"gamePk\", how=\"left\")\n",
    "\n",
    "print(\"home_sp_throws non-null:\", df_all[\"home_sp_throws\"].notnull().sum(), \"/\", len(df_all))\n",
    "print(\"away_sp_throws non-null:\", df_all[\"away_sp_throws\"].notnull().sum(), \"/\", len(df_all))\n",
    "\n",
    "\n",
    "# --- Optional: drop games before April 7 in every season ---\n",
    "df_all[\"gameDate_dt\"] = pd.to_datetime(df_all[\"gameDate\"], utc=True, errors=\"coerce\")\n",
    "cutoff_month, cutoff_day = 4, 7\n",
    "\n",
    "mask_cutoff = ~(\n",
    "    (df_all[\"gameDate_dt\"].dt.month < cutoff_month) |\n",
    "    ((df_all[\"gameDate_dt\"].dt.month == cutoff_month) & (df_all[\"gameDate_dt\"].dt.day < cutoff_day))\n",
    ")\n",
    "\n",
    "df_filt2 = df_all.loc[mask_cutoff].copy()\n",
    "print(\"Rows after Apr 7 cutoff (all seasons):\", len(df_filt2))\n",
    "\n",
    "print(df_filt2.columns.tolist())\n",
    "\n",
    "# --- Starter handedness features (context, not home/away-diff pairs) ---\n",
    "# Encode L/R as binary; treat missing/unknown as 0 (or impute later via SimpleImputer)\n",
    "df_filt2[\"home_sp_left\"] = (df_filt2[\"home_sp_throws\"].astype(str).str.upper() == \"L\").astype(int)\n",
    "df_filt2[\"away_sp_left\"] = (df_filt2[\"away_sp_throws\"].astype(str).str.upper() == \"L\").astype(int)\n",
    "\n",
    "# OPS sources you already have\n",
    "HOME_SLG_L10 = \"home_bat_last10_B8_SLG_mean\"\n",
    "AWAY_SLG_L10 = \"away_bat_last10_B8_SLG_mean\"\n",
    "\n",
    "df_filt2[\"home_slg_x_away_sp_left\"] = df_filt2[HOME_SLG_L10] * df_filt2[\"away_sp_left\"]\n",
    "df_filt2[\"away_slg_x_home_sp_left\"] = df_filt2[AWAY_SLG_L10] * df_filt2[\"home_sp_left\"]\n",
    "df_filt2[\"diff_slg_x_sp_left\"] = (\n",
    "    df_filt2[\"home_slg_x_away_sp_left\"]\n",
    "    - df_filt2[\"away_slg_x_home_sp_left\"]\n",
    ")\n",
    "\n",
    "# A compact single feature capturing relative platoon direction\n",
    "# (-1 means away starter is L and home is not; +1 means home starter is L and away is not)\n",
    "df_filt2[\"diff_sp_left\"] = df_filt2[\"home_sp_left\"] - df_filt2[\"away_sp_left\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7018cabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns: 58\n",
      "  - B14-B17: 48\n",
      "  - sp_career SP8/SP9: 4\n",
      "  - meta: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>homeWin</th>\n",
       "      <th>home_bat_season_B1_AB_mean</th>\n",
       "      <th>home_bat_season_B1_AB_std</th>\n",
       "      <th>home_bat_season_B2_H_mean</th>\n",
       "      <th>home_bat_season_B2_H_std</th>\n",
       "      <th>home_bat_season_B3_BB_mean</th>\n",
       "      <th>home_bat_season_B3_BB_std</th>\n",
       "      <th>home_bat_season_B4_SO_mean</th>\n",
       "      <th>home_bat_season_B4_SO_std</th>\n",
       "      <th>...</th>\n",
       "      <th>park_pf_runs</th>\n",
       "      <th>home_sp_throws</th>\n",
       "      <th>away_sp_throws</th>\n",
       "      <th>gameDate_dt</th>\n",
       "      <th>home_sp_left</th>\n",
       "      <th>away_sp_left</th>\n",
       "      <th>home_slg_x_away_sp_left</th>\n",
       "      <th>away_slg_x_home_sp_left</th>\n",
       "      <th>diff_slg_x_sp_left</th>\n",
       "      <th>diff_sp_left</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.373</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "      <td>2015-04-07 02:05:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>107.453</td>\n",
       "      <td>R</td>\n",
       "      <td>L</td>\n",
       "      <td>2015-04-07 02:10:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>91.084</td>\n",
       "      <td>R</td>\n",
       "      <td>L</td>\n",
       "      <td>2015-04-07 23:10:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>90.310</td>\n",
       "      <td>R</td>\n",
       "      <td>L</td>\n",
       "      <td>2015-04-07 23:10:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>106.246</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "      <td>2015-04-08 00:10:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 308 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    season  homeWin  home_bat_season_B1_AB_mean  home_bat_season_B1_AB_std  \\\n",
       "13    2015      1.0                        -1.0                       -1.0   \n",
       "14    2015      0.0                        -1.0                       -1.0   \n",
       "15    2015      0.0                        31.0                        0.0   \n",
       "16    2015      0.0                        32.0                        0.0   \n",
       "17    2015      0.0                        33.0                        0.0   \n",
       "\n",
       "    home_bat_season_B2_H_mean  home_bat_season_B2_H_std  \\\n",
       "13                       -1.0                      -1.0   \n",
       "14                       -1.0                      -1.0   \n",
       "15                        8.0                       0.0   \n",
       "16                        8.0                       0.0   \n",
       "17                        8.0                       0.0   \n",
       "\n",
       "    home_bat_season_B3_BB_mean  home_bat_season_B3_BB_std  \\\n",
       "13                        -1.0                       -1.0   \n",
       "14                        -1.0                       -1.0   \n",
       "15                         1.0                        0.0   \n",
       "16                         3.0                        0.0   \n",
       "17                         0.0                        0.0   \n",
       "\n",
       "    home_bat_season_B4_SO_mean  home_bat_season_B4_SO_std  ...  park_pf_runs  \\\n",
       "13                        -1.0                       -1.0  ...       100.373   \n",
       "14                        -1.0                       -1.0  ...       107.453   \n",
       "15                         7.0                        0.0  ...        91.084   \n",
       "16                         8.0                        0.0  ...        90.310   \n",
       "17                         9.0                        0.0  ...       106.246   \n",
       "\n",
       "    home_sp_throws  away_sp_throws               gameDate_dt  home_sp_left  \\\n",
       "13               R               R 2015-04-07 02:05:00+00:00             0   \n",
       "14               R               L 2015-04-07 02:10:00+00:00             0   \n",
       "15               R               L 2015-04-07 23:10:00+00:00             0   \n",
       "16               R               L 2015-04-07 23:10:00+00:00             0   \n",
       "17               R               R 2015-04-08 00:10:00+00:00             0   \n",
       "\n",
       "    away_sp_left  home_slg_x_away_sp_left  away_slg_x_home_sp_left  \\\n",
       "13             0                     -0.0                     -0.0   \n",
       "14             1                     -1.0                     -0.0   \n",
       "15             1                     -1.0                     -0.0   \n",
       "16             1                     -1.0                     -0.0   \n",
       "17             0                     -0.0                     -0.0   \n",
       "\n",
       "    diff_slg_x_sp_left  diff_sp_left  \n",
       "13                 0.0             0  \n",
       "14                -1.0            -1  \n",
       "15                -1.0            -1  \n",
       "16                -1.0            -1  \n",
       "17                 0.0             0  \n",
       "\n",
       "[5 rows x 308 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Drop columns you don't want (B14–B17 everywhere; career SP8/SP9; metadata)\n",
    "\n",
    "# Drop B14-B17 columns anywhere in the dataframe (means/stds/blocks)\n",
    "drop_b_cols = [c for c in df_filt2.columns if any(x in c for x in [\"B14_\", \"B15_\", \"B16_\", \"B17_\"])]\n",
    "\n",
    "# Drop career SP8/SP9 columns only (we keep other career fields)\n",
    "drop_sp_career_cols = [\n",
    "    c for c in df_filt2.columns\n",
    "    if (\"sp_career_\" in c) and ((\"SP8_\" in c) or (\"SP9_\" in c))\n",
    "]\n",
    "\n",
    "# Drop non-feature columns (IDs/names/dates)\n",
    "meta_cols = [\n",
    "    \"gamePk\", \"gameDate\",\n",
    "    \"homeTeamName\", \"awayTeamName\",\n",
    "    \"homeTeamId\", \"awayTeamId\",\n",
    "]\n",
    "\n",
    "to_drop = drop_b_cols + drop_sp_career_cols + meta_cols\n",
    "print(\"Dropping columns:\", len(to_drop))\n",
    "print(\"  - B14-B17:\", len(drop_b_cols))\n",
    "print(\"  - sp_career SP8/SP9:\", len(drop_sp_career_cols))\n",
    "print(\"  - meta:\", len(meta_cols))\n",
    "\n",
    "df_model = df_filt2.drop(columns=to_drop)\n",
    "df_model.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6b62afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired features: 150\n",
      "Diffed numeric pairs: 149\n",
      "Skipped non-numeric pairs: 1\n",
      "Examples skipped: ['sp_throws']\n",
      "Shape after differencing: (24437, 154)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n",
      "/var/folders/cf/4zyl4dsd16n9yhd_dkhk66gc0000gn/T/ipykernel_97675/849842142.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_diff[f\"diff_{base}\"] = h_num - a_num\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>homeWin</th>\n",
       "      <th>park_pf_runs</th>\n",
       "      <th>gameDate_dt</th>\n",
       "      <th>diff_slg_x_sp_left</th>\n",
       "      <th>diff_sp_left</th>\n",
       "      <th>diff_bat_season_B1_AB_mean</th>\n",
       "      <th>diff_bat_season_B1_AB_std</th>\n",
       "      <th>diff_bat_season_B2_H_mean</th>\n",
       "      <th>diff_bat_season_B2_H_std</th>\n",
       "      <th>...</th>\n",
       "      <th>diff_rp_season_P18_IS_mean</th>\n",
       "      <th>diff_rp_season_P18_IS_std</th>\n",
       "      <th>diff_sp_career_SP1_IP</th>\n",
       "      <th>diff_sp_career_SP2_H</th>\n",
       "      <th>diff_sp_career_SP3_BB</th>\n",
       "      <th>diff_sp_career_SP4_SO</th>\n",
       "      <th>diff_sp_career_SP5_HR</th>\n",
       "      <th>diff_sp_career_SP6_ERA</th>\n",
       "      <th>diff_sp_career_SP7_BF</th>\n",
       "      <th>diff_sp_career_WHIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.373</td>\n",
       "      <td>2015-04-07 02:05:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.333333</td>\n",
       "      <td>-95.0</td>\n",
       "      <td>-89.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>159.0</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.453</td>\n",
       "      <td>2015-04-07 02:10:00+00:00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1514.000000</td>\n",
       "      <td>-1342.0</td>\n",
       "      <td>-376.0</td>\n",
       "      <td>-1576.0</td>\n",
       "      <td>-170.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-6240.0</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.084</td>\n",
       "      <td>2015-04-07 23:10:00+00:00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-104.666667</td>\n",
       "      <td>-169.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>-162.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-510.0</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 154 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    season  homeWin  park_pf_runs               gameDate_dt  \\\n",
       "13    2015      1.0       100.373 2015-04-07 02:05:00+00:00   \n",
       "14    2015      0.0       107.453 2015-04-07 02:10:00+00:00   \n",
       "15    2015      0.0        91.084 2015-04-07 23:10:00+00:00   \n",
       "\n",
       "    diff_slg_x_sp_left  diff_sp_left  diff_bat_season_B1_AB_mean  \\\n",
       "13                 0.0             0                         0.0   \n",
       "14                -1.0            -1                         0.0   \n",
       "15                -1.0            -1                        -2.0   \n",
       "\n",
       "    diff_bat_season_B1_AB_std  diff_bat_season_B2_H_mean  \\\n",
       "13                        0.0                        0.0   \n",
       "14                        0.0                        0.0   \n",
       "15                        0.0                        2.0   \n",
       "\n",
       "    diff_bat_season_B2_H_std  ...  diff_rp_season_P18_IS_mean  \\\n",
       "13                       0.0  ...                         0.0   \n",
       "14                       0.0  ...                         0.0   \n",
       "15                       0.0  ...                         0.0   \n",
       "\n",
       "    diff_rp_season_P18_IS_std  diff_sp_career_SP1_IP  diff_sp_career_SP2_H  \\\n",
       "13                        0.0             101.333333                 -95.0   \n",
       "14                        0.0           -1514.000000               -1342.0   \n",
       "15                        0.0            -104.666667                -169.0   \n",
       "\n",
       "    diff_sp_career_SP3_BB  diff_sp_career_SP4_SO  diff_sp_career_SP5_HR  \\\n",
       "13                  -89.0                  341.0                  -13.0   \n",
       "14                 -376.0                -1576.0                 -170.0   \n",
       "15                  -21.0                 -162.0                  -11.0   \n",
       "\n",
       "    diff_sp_career_SP6_ERA  diff_sp_career_SP7_BF  diff_sp_career_WHIP  \n",
       "13                   -0.48                  159.0                -0.17  \n",
       "14                    0.17                -6240.0                 0.05  \n",
       "15                   -0.14                 -510.0                -0.05  \n",
       "\n",
       "[3 rows x 154 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Home–away differencing ---\n",
    "import re\n",
    "\n",
    "df_diff = df_model.copy()\n",
    "\n",
    "home_cols = [c for c in df_diff.columns if c.startswith(\"home_\")]\n",
    "away_cols = [c for c in df_diff.columns if c.startswith(\"away_\")]\n",
    "\n",
    "# Map base feature name -> (home_col, away_col)\n",
    "pairs = {}\n",
    "for h in home_cols:\n",
    "    base = h.replace(\"home_\", \"\")\n",
    "    a = \"away_\" + base\n",
    "    if a in away_cols:\n",
    "        pairs[base] = (h, a)\n",
    "\n",
    "print(\"Paired features:\", len(pairs))\n",
    "\n",
    "skipped = []\n",
    "diffed = 0\n",
    "\n",
    "# Create differenced features\n",
    "for base, (h, a) in pairs.items():\n",
    "    # Only diff if both columns are numeric (or can be safely coerced)\n",
    "    h_num = pd.to_numeric(df_diff[h], errors=\"coerce\")\n",
    "    a_num = pd.to_numeric(df_diff[a], errors=\"coerce\")\n",
    "\n",
    "    # If both were originally non-numeric, coercion will be mostly NaN -> skip\n",
    "    # Heuristic: require at least some non-NaN in BOTH after coercion\n",
    "    if h_num.notna().sum() == 0 or a_num.notna().sum() == 0:\n",
    "        skipped.append(base)\n",
    "        continue\n",
    "\n",
    "    df_diff[f\"diff_{base}\"] = h_num - a_num\n",
    "    diffed += 1\n",
    "\n",
    "\n",
    "# Drop original home/away columns (whether diffed or skipped)\n",
    "df_diff = df_diff.drop(columns=home_cols + away_cols)\n",
    "\n",
    "print(\"Diffed numeric pairs:\", diffed)\n",
    "print(\"Skipped non-numeric pairs:\", len(skipped))\n",
    "if skipped:\n",
    "    print(\"Examples skipped:\", skipped[:20])\n",
    "\n",
    "print(\"Shape after differencing:\", df_diff.shape)\n",
    "df_diff.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "805215b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_diff[\"wind_mph_clip\"] = df_diff[\"wind_mph\"].clip(0, 25)\n",
    "# df_diff[\"humidity_clip\"] = df_diff[\"humidity\"].clip(0, 100)\n",
    "# df_diff[\"precip_mm_clip\"] = df_diff[\"precip_mm\"].clip(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "56bf8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (24437, 152)\n",
      "y shape: (24437,)\n"
     ]
    }
   ],
   "source": [
    "# Step 4 (differenced): Build X / y\n",
    "import numpy as np\n",
    "\n",
    "y = df_diff[\"homeWin\"].astype(float).values\n",
    "X = df_diff.drop(columns=[\"homeWin\"])\n",
    "\n",
    "# Drop datetime columns if present\n",
    "dt_cols = X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns.tolist()\n",
    "if dt_cols:\n",
    "    X = X.drop(columns=dt_cols)\n",
    "\n",
    "# Drop any non-numeric columns (safety)\n",
    "non_num = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_num:\n",
    "    X = X.drop(columns=non_num)\n",
    "\n",
    "X = X.astype(\"float32\")\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6b8c9ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 19549\n",
      "Val rows: 4888\n",
      "Train homeWin mean: 0.5340938155404369\n",
      "Val homeWin mean: 0.5317103109656302\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Time-based train/validation split (80/20 on post-cutoff data)\n",
    "n = len(X)\n",
    "train_end = int(0.80 * n)\n",
    "\n",
    "X_train, y_train = X.iloc[:train_end], y[:train_end]\n",
    "X_val, y_val     = X.iloc[train_end:], y[train_end:]\n",
    "\n",
    "print(\"Train rows:\", len(X_train))\n",
    "print(\"Val rows:\", len(X_val))\n",
    "print(\"Train homeWin mean:\", y_train.mean())\n",
    "print(\"Val homeWin mean:\", y_val.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d51e809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features before: 152\n",
      "Features after stage 1: 59\n"
     ]
    }
   ],
   "source": [
    "# ---- Stage 1: manual pruning ----\n",
    "import re\n",
    "\n",
    "cols = X_train.columns.tolist()\n",
    "\n",
    "drop_patterns = [\n",
    "    \"_std\",                # drop all stds\n",
    "    \"last20\",              # drop last20 windows\n",
    "    # \"bat_season\",          # optional: comment out if you want season batting\n",
    "    \"sp_season\",           # drop season SP stats\n",
    "]\n",
    "\n",
    "def should_drop(c):\n",
    "    return any(p in c for p in drop_patterns)\n",
    "\n",
    "keep_cols = [c for c in cols if not should_drop(c)]\n",
    "\n",
    "Xtr_1 = X_train[keep_cols]\n",
    "Xva_1 = X_val[keep_cols]\n",
    "\n",
    "print(\"Features before:\", X_train.shape[1])\n",
    "print(\"Features after stage 1:\", Xtr_1.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4254d06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped due to correlation: 11\n",
      "Remaining features: 48\n"
     ]
    }
   ],
   "source": [
    "# ---- Stage 2: correlation pruning ----\n",
    "import numpy as np\n",
    "\n",
    "corr = Xtr_1.corr().abs()\n",
    "\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "\n",
    "Xtr_2 = Xtr_1.drop(columns=to_drop)\n",
    "Xva_2 = Xva_1.drop(columns=to_drop)\n",
    "\n",
    "print(\"Dropped due to correlation:\", len(to_drop))\n",
    "print(\"Remaining features:\", Xtr_2.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "05d0ecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: 40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "l1 = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        solver=\"saga\",\n",
    "        C=0.1,        # try 0.05–0.2\n",
    "        max_iter=4000\n",
    "    ))\n",
    "])\n",
    "\n",
    "l1.fit(Xtr_2, y_train)\n",
    "\n",
    "coef = l1.named_steps[\"clf\"].coef_[0]\n",
    "selected = np.abs(coef) > 1e-6\n",
    "\n",
    "selected_cols = Xtr_2.columns[selected]\n",
    "print(\"Selected features:\", len(selected_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a0d8f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final pruned LogReg AUC: 0.6023358784194816\n",
      "Final pruned LogReg ACC: 0.5752864157119476\n",
      "Used cols: 40 | includes diff_sp_left = True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# --- ensure handedness feature is included (even if L1 drops it) ---\n",
    "selected_cols_final = list(selected_cols)\n",
    "# for c in [\"diff_sp_left\", \"diff_slg_x_sp_left\", \"park_pf_runs\"]:\n",
    "# for c in [\"diff_sp_left\", \"park_pf_runs\"]:\n",
    "for c in [\"park_pf_runs\"]:\n",
    "    if c in Xtr_2.columns and c not in selected_cols_final:\n",
    "        selected_cols_final.append(c)\n",
    "\n",
    "logreg_diff = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "Xtr_3 = Xtr_2[selected_cols_final]\n",
    "Xva_3 = Xva_2[selected_cols_final]\n",
    "\n",
    "logreg_diff.fit(Xtr_3, y_train)\n",
    "pred = logreg_diff.predict_proba(Xva_3)[:,1]\n",
    "\n",
    "print(\"Final pruned LogReg AUC:\", roc_auc_score(y_val, pred))\n",
    "print(\"Final pruned LogReg ACC:\", accuracy_score(y_val, pred > 0.5))\n",
    "print(\"Used cols:\", len(selected_cols_final), \"| includes diff_sp_left =\", \"diff_sp_left\" in selected_cols_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "aa5e65b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 1s 3ms/step - loss: 0.7732 - auc: 0.5231 - accuracy: 0.5232 - val_loss: 0.6886 - val_auc: 0.5570 - val_accuracy: 0.5319\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.7243 - auc: 0.5401 - accuracy: 0.5335 - val_loss: 0.6863 - val_auc: 0.5769 - val_accuracy: 0.5417\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7100 - auc: 0.5494 - accuracy: 0.5388 - val_loss: 0.6853 - val_auc: 0.5806 - val_accuracy: 0.5442\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7020 - auc: 0.5532 - accuracy: 0.5411 - val_loss: 0.6819 - val_auc: 0.5845 - val_accuracy: 0.5675\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6970 - auc: 0.5584 - accuracy: 0.5494 - val_loss: 0.6856 - val_auc: 0.5861 - val_accuracy: 0.5473\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6894 - auc: 0.5715 - accuracy: 0.5576 - val_loss: 0.6890 - val_auc: 0.5946 - val_accuracy: 0.5395\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6896 - auc: 0.5659 - accuracy: 0.5500 - val_loss: 0.6820 - val_auc: 0.5936 - val_accuracy: 0.5573\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6854 - auc: 0.5764 - accuracy: 0.5612 - val_loss: 0.6813 - val_auc: 0.5928 - val_accuracy: 0.5587\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6827 - auc: 0.5825 - accuracy: 0.5662 - val_loss: 0.6926 - val_auc: 0.5978 - val_accuracy: 0.5415\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6829 - auc: 0.5804 - accuracy: 0.5612 - val_loss: 0.6841 - val_auc: 0.5864 - val_accuracy: 0.5538\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6809 - auc: 0.5851 - accuracy: 0.5660 - val_loss: 0.6767 - val_auc: 0.5933 - val_accuracy: 0.5651\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6790 - auc: 0.5902 - accuracy: 0.5704 - val_loss: 0.6806 - val_auc: 0.5971 - val_accuracy: 0.5669\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6792 - auc: 0.5896 - accuracy: 0.5718 - val_loss: 0.6791 - val_auc: 0.5953 - val_accuracy: 0.5628\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6780 - auc: 0.5914 - accuracy: 0.5734 - val_loss: 0.6766 - val_auc: 0.5981 - val_accuracy: 0.5687\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6779 - auc: 0.5915 - accuracy: 0.5728 - val_loss: 0.6781 - val_auc: 0.6032 - val_accuracy: 0.5636\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6769 - auc: 0.5963 - accuracy: 0.5742 - val_loss: 0.6756 - val_auc: 0.6001 - val_accuracy: 0.5710\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6767 - auc: 0.5949 - accuracy: 0.5718 - val_loss: 0.6753 - val_auc: 0.6000 - val_accuracy: 0.5722\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6758 - auc: 0.5982 - accuracy: 0.5761 - val_loss: 0.6769 - val_auc: 0.6021 - val_accuracy: 0.5669\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6754 - auc: 0.6007 - accuracy: 0.5792 - val_loss: 0.6755 - val_auc: 0.6016 - val_accuracy: 0.5753\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6759 - auc: 0.5987 - accuracy: 0.5739 - val_loss: 0.6748 - val_auc: 0.6019 - val_accuracy: 0.5743\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6745 - auc: 0.6025 - accuracy: 0.5773 - val_loss: 0.6781 - val_auc: 0.6012 - val_accuracy: 0.5634\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6754 - auc: 0.5994 - accuracy: 0.5756 - val_loss: 0.6757 - val_auc: 0.6016 - val_accuracy: 0.5716\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6755 - auc: 0.5989 - accuracy: 0.5753 - val_loss: 0.6750 - val_auc: 0.6025 - val_accuracy: 0.5765\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6746 - auc: 0.6015 - accuracy: 0.5785 - val_loss: 0.6750 - val_auc: 0.6031 - val_accuracy: 0.5743\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6746 - auc: 0.6029 - accuracy: 0.5802 - val_loss: 0.6755 - val_auc: 0.6011 - val_accuracy: 0.5726\n",
      "ANN AUC: 0.6031\n",
      "ANN ACC: 0.5636\n"
     ]
    }
   ],
   "source": [
    "# ANN baseline (paper-aligned scaling: MinMax) for clean comparison\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "Xtr_imp = imputer.fit_transform(Xtr_3.values)\n",
    "Xva_imp = imputer.transform(Xva_3.values)\n",
    "\n",
    "# 1) Scale (fit on train only)\n",
    "scaler = MinMaxScaler()\n",
    "Xtr_ann = scaler.fit_transform(Xtr_imp)\n",
    "Xva_ann = scaler.transform(Xva_imp)\n",
    "\n",
    "ytr = y_train.astype(\"float32\")\n",
    "yva = y_val.astype(\"float32\")\n",
    "\n",
    "# 2) Build a compact MLP (keep it small to avoid overfitting)\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(Xtr_ann.shape[1],), activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    "\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[tf.keras.metrics.AUC(name=\"auc\"), \"accuracy\"]\n",
    ")\n",
    "\n",
    "# 3) Train with early stopping\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_auc\",\n",
    "    mode=\"max\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "hist = model.fit(\n",
    "    Xtr_ann, ytr,\n",
    "    validation_data=(Xva_ann, yva),\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4) Evaluate\n",
    "pred = model.predict(Xva_ann, verbose=0).reshape(-1)\n",
    "auc = roc_auc_score(yva, pred)\n",
    "acc = accuracy_score(yva, pred > 0.5)\n",
    "\n",
    "print(\"ANN AUC:\", round(auc, 4))\n",
    "print(\"ANN ACC:\", round(acc, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4774d9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['season',\n",
       " 'park_pf_runs',\n",
       " 'diff_slg_x_sp_left',\n",
       " 'diff_sp_left',\n",
       " 'diff_bat_season_B1_AB_mean',\n",
       " 'diff_bat_season_B2_H_mean',\n",
       " 'diff_bat_season_B3_BB_mean',\n",
       " 'diff_bat_season_B4_SO_mean',\n",
       " 'diff_bat_season_B7_OBP_mean',\n",
       " 'diff_bat_season_B8_SLG_mean',\n",
       " 'diff_bat_season_B10_Pit_mean',\n",
       " 'diff_bat_season_B11_Str_mean',\n",
       " 'diff_bat_season_B12_PO_mean',\n",
       " 'diff_bat_season_B13_A_mean',\n",
       " 'diff_bat_last10_B1_AB_mean',\n",
       " 'diff_bat_last10_B2_H_mean',\n",
       " 'diff_bat_last10_B4_SO_mean',\n",
       " 'diff_bat_last10_B10_Pit_mean',\n",
       " 'diff_bat_last10_B11_Str_mean',\n",
       " 'diff_bat_last10_B13_A_mean',\n",
       " 'diff_sp_last3_SP1_IP_mean',\n",
       " 'diff_sp_last3_SP2_H_mean',\n",
       " 'diff_sp_last3_SP3_BB_mean',\n",
       " 'diff_sp_last3_SP4_SO_mean',\n",
       " 'diff_sp_last3_SP5_HR_mean',\n",
       " 'diff_sp_last3_SP7_BF_mean',\n",
       " 'diff_sp_last3_SP9_Str_mean',\n",
       " 'diff_sp_last3_WHIP_mean',\n",
       " 'diff_rp_season_P2_H_mean',\n",
       " 'diff_rp_season_P3_BB_mean',\n",
       " 'diff_rp_season_P4_SO_mean',\n",
       " 'diff_rp_season_P5_HR_mean',\n",
       " 'diff_rp_season_P6_ERA_mean',\n",
       " 'diff_rp_season_P7_BF_mean',\n",
       " 'diff_rp_season_P17_IR_mean',\n",
       " 'diff_rp_season_P18_IS_mean',\n",
       " 'diff_sp_career_SP1_IP',\n",
       " 'diff_sp_career_SP3_BB',\n",
       " 'diff_sp_career_SP6_ERA',\n",
       " 'diff_sp_career_WHIP']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_cols_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
